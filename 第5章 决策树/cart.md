##CART算法
CART由特征选择，树的生成，剪枝组成。可以用于分类也可用于回归。
CART假设决策树是二叉树，特征的取值为是或否，给定输入随机变量x条件下输出随机变量y的概率分布。
组成：
1.基于大量的数据生成决策树，尽可能的大
2.用验证数据，通过损失函数最小进行剪枝选择最优子树。

#### CART的生成
对于回归问题用平方误差最小化标准进行特征选择，对于分类问题用基尼系数进行特征选择。
* 回归树的生成
对于回归树问题，将其输入空间进行划分，通过计算划分后，不同划分单元的输出值（这里讲的输出值就是每个划分单元的均值），在通过均值和划分单元计算其平方误差来找到最优的划分点，再通过迭代的方式构建最小二乘法回归树。
$$min_{(j,s)} \left[ min_{c_1} \sum_{x_i}(y_i - c_1)^2 +min_{c_2} \sum_{x_i}(y_i - c_2)^2  \right]$$
其中j属于划分点 s是划分点对应的值，括号内第一个min是对于划分1类别，c1是划分1类别的均值，括号内第二个min是对于划分2类别，c2是划分2类别的均值。
* 分类树的生成
  CART分类树算法使用基尼系数来代替信息增益比，基尼系数代表了模型的不纯度，基尼系数越小，则不纯度越低，特征越好。这和信息增益(比)是相反的。具体的，在分类问题中，假设有K个类别，第k个类别的概率为pk, 则基尼系数的表达式为：

$$Gini(p)=∑_{k=1}^Kp_k(1−p_k)=1−∑_{k=1}^Kp^2_k$$
　　　　如果是二类分类问题，计算就更加简单了，如果属于第一个样本输出的概率是p，则基尼系数的表达式为：

$$Gini(p)=2p(1−p)$$
　　　　对于个给定的样本D,假设有K个类别, 第k个类别的数量为Ck,则样本D的基尼系数表达式为：

$$Gini(D)=1−∑_{k=1}^K(\frac{|Ck|}{|D|})^2$$
 

　　　　特别的，对于样本D,如果根据特征A的某个值a,把D分成D1和D2两部分，则在特征A的条件下，D的基尼系数表达式为：

$$Gini(D,A)=\frac{|D1|}{|D|}Gini(D1)+\frac{|D2|}{|D|}Gini(D2)$$
对于二类分类，基尼系数和熵之半的曲线如下：
  ![image.png](https://upload-images.jianshu.io/upload_images/3426235-ce0db8677549b05d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
　　　从上图可以看出，基尼系数和熵之半的曲线非常接近，仅仅在45度角附近误差稍大。因此，基尼系数可以做为熵模型的一个近似替代。而CART分类树算法就是使用的基尼系数来选择决策树的特征。同时，为了进一步简化，CART分类树算法每次仅仅对某个特征的值进行二分，而不是多分，这样CART分类树算法建立起来的是二叉树，而不是多叉树。这样一可以进一步简化基尼系数的计算，二可以建立一个更加优雅的二叉树模型。
基尼指数越大，样本集合的不确定性也就越大。

## 剪枝
待定，还没看懂。。。。


























