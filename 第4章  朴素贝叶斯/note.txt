# 朴素贝叶斯(naive bayes) 基于贝叶斯定理和特征条件独立假设的分类方法
### 章节目录

* 朴素贝叶斯的学习和分类
     
      1.基本方法
      2.后验概率最大化的含义
        
* 朴素贝叶斯的参数估计
    
      1.极大似然估计
      2.学习与分类算法
      3.贝叶斯估计

#### 朴素贝叶斯
在书中：
 > 条件概率分布P(X=x|Y=ck)有指数级数量的参数，其实际估计是不可行的
 
1.指数级数量的参数，这点书后面解释了，$K\prod_{j=1}^nS_j$，这个为什么是指数级数量，假设$S_j=S$，表达式变为$KS^n$
2.实际估计是不可行的 ：估计这么多参数需要更多的样本来刷参数，实际上获取这么多样本是不可行的。

朴素贝叶斯法：
* 贝叶斯定理
* 特征条件独立假设

贝叶斯定理
$$P(Y=c_k|X=x) = \frac{P(X=x|Y=c_k)P(Y=c_k)}{\sum{k}P(Y=c_k)P(X=x|Y=c_k)}$$

条件独立假设等于是说用于分类的特征在类确定的条件下都是条件独立的。 

期望风险最小化(即损失最小化)等价于后验概率最大化

学习与分类算法：
    1.使用最大似然估计法估计P(Y=ck)和$P(X^{(j)}=x^(j)|Y=c_k)$,通过贝叶斯选择器$y=argmax_{c_k}P(Y=c_k)\sum^n_{j=1}P(X^{(j)}=x^(j)|Y=c_k)$
    2.由于使用最大似然估计会产生所有概率为0 的情况，影响后续的计算。解决办法：采用贝叶斯估计
    3.贝叶斯估计下的条件概率：$$ P_{\lambda}(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum\limits_{i=1}^NI(x_i^{j}=a_{jl},y_j=c_k)+\lambda}{\sum\limits_{i=1}^NI(y_j=c_k)+S_j\lambda} $$
    
   4.其中$\lambda \geqslant 0$ 当$\lambda = 0$的时候，就是极大似然估计。当$\lambda=1$的时候，这个平滑方案叫做Laplace Smoothing。拉普拉斯平滑相当于给未知变量给定了先验概率。
    
